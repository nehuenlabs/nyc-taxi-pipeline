# NYC Taxi Mini Pipeline

[![CI](https://github.com/nehuenlabs/nyc-taxi-pipeline/workflows/CI/badge.svg)](https://github.com/nehuenlabs/nyc-taxi-pipeline/actions)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A data pipeline for processing NYC Taxi data using PySpark, loading into a dimensional model, and infrastructure provisioned with Terraform on GCP.

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         NYC TAXI PIPELINE                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ  Raw    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Bronze  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Gold   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   PostgreSQL    ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ (NYC    ‚îÇ     ‚îÇ (Raw +  ‚îÇ     ‚îÇ (Star   ‚îÇ     ‚îÇ   / BigQuery    ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  TLC)   ‚îÇ     ‚îÇ  Meta)  ‚îÇ     ‚îÇ Schema) ‚îÇ     ‚îÇ                 ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ   ‚îÇ                    Development: Docker                           ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                    Production:  GCP (Terraform)                  ‚îÇ  ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìã Table of Contents

- [Features](#-features)
- [Prerequisites](#-prerequisites)
- [Quick Start](#-quick-start)
- [Project Structure](#-project-structure)
- [Data Model](#-data-model)
- [Pipeline Jobs](#-pipeline-jobs)
- [Configuration](#-configuration)
- [Terraform Infrastructure](#-terraform-infrastructure)
- [Testing](#-testing)
- [CI/CD](#-cicd)
- [Historical Data Strategy](#-historical-data-strategy)
- [Limitations & Future Improvements](#-limitations--future-improvements)

## ‚ú® Features

- **Medallion Architecture**: Bronze ‚Üí Silver ‚Üí Gold data layers
- **Dimensional Modeling**: Star schema with fact and dimension tables
- **Environment Agnostic**: Same code runs locally (Docker) and on GCP
- **Infrastructure as Code**: Terraform modules for GCP resources
- **Historical Data Support**: Backfill and re-process specific months
- **Idempotent Processing**: Partition overwrite prevents duplicates
- **Comprehensive Testing**: Unit and integration tests with pytest
- **CI/CD Pipeline**: GitHub Actions for automated validation

## üì¶ Prerequisites

- **Docker** & **Docker Compose** (v2.0+)
- **Python** 3.10+ (for local development)
- **Make** (optional, for convenience commands)
- **Terraform** 1.0+ (for GCP infrastructure)

For GCP deployment (optional):
- GCP Account with billing enabled
- `gcloud` CLI configured

## üöÄ Quick Start

### 1. Clone and Setup

```bash
git clone https://github.com/marcelo/nyc-taxi-pipeline.git
cd nyc-taxi-pipeline

# Initial setup (creates directories, installs dependencies)
make setup
```

### 2. Start Services

```bash
# Start Spark + PostgreSQL
make start

# Verify services are running
make status
```

### 3. Run the Pipeline

```bash
# Download sample data (January 2023)
make download-data MONTHS=2023-01

# Run full pipeline
make run-full MONTHS=2023-01
```

### 4. Verify Results

- **Spark UI**: http://localhost:8081
- **Adminer (DB UI)**: http://localhost:8080
  - System: PostgreSQL
  - Server: postgres
  - User: pipeline
  - Password: pipeline123
  - Database: nyctaxi

## üìÅ Project Structure

```
nyc-taxi-pipeline/
‚îú‚îÄ‚îÄ src/                    # Source code
‚îÇ   ‚îú‚îÄ‚îÄ jobs/              # PySpark jobs
‚îÇ   ‚îú‚îÄ‚îÄ models/            # Data schemas & dimensional model
‚îÇ   ‚îú‚îÄ‚îÄ utils/             # Configuration & utilities
‚îÇ   ‚îî‚îÄ‚îÄ quality/           # Data quality validators
‚îú‚îÄ‚îÄ tests/                 # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ unit/             # Unit tests
‚îÇ   ‚îî‚îÄ‚îÄ integration/      # Integration tests
‚îú‚îÄ‚îÄ terraform/             # GCP infrastructure
‚îÇ   ‚îî‚îÄ‚îÄ modules/          # Terraform modules (GCS, IAM, BigQuery)
‚îú‚îÄ‚îÄ docker/               # Docker configuration
‚îú‚îÄ‚îÄ sql/                  # SQL scripts
‚îÇ   ‚îú‚îÄ‚îÄ postgres/        # PostgreSQL DDL
‚îÇ   ‚îî‚îÄ‚îÄ bigquery/        # BigQuery DDL
‚îú‚îÄ‚îÄ scripts/              # Utility scripts
‚îú‚îÄ‚îÄ data/                 # Local data (gitignored)
‚îî‚îÄ‚îÄ docs/                 # Documentation
```

## üìä Data Model

### Star Schema

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ    dim_date     ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  dim_location   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ   dim_vendor    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   fact_trips    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  dim_ratecode   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ dim_payment_type‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ    dim_time     ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üîß Pipeline Jobs

### Job 1: Bronze Ingestion

```bash
make run-bronze MONTHS=2023-01
```

- Downloads raw Parquet data from NYC TLC
- Adds metadata columns (ingestion timestamp, source file)
- Partitions by year/month
- Stores in Bronze layer

### Job 2: Dimensional Transform

```bash
make run-transform MONTHS=2023-01
```

- Reads from Bronze layer
- Cleans and validates data
- Joins with zone lookup
- Creates fact and dimension tables
- Loads into PostgreSQL

## ‚öôÔ∏è Configuration

Environment variables (`.env`):

| Variable | Description | Default |
|----------|-------------|---------|
| `PIPELINE_ENV` | Environment (local/gcp) | `local` |
| `POSTGRES_HOST` | PostgreSQL host | `postgres` |
| `POSTGRES_DB` | Database name | `nyctaxi` |
| `GCS_BUCKET` | GCS bucket (GCP only) | - |
| `GCP_PROJECT_ID` | GCP project (GCP only) | - |

## üèóÔ∏è Terraform Infrastructure

### Structure

```
terraform/
‚îú‚îÄ‚îÄ main.tf              # GCS buckets, Service Account, IAM bindings
‚îú‚îÄ‚îÄ variables.tf         # Input variables
‚îú‚îÄ‚îÄ outputs.tf          # Output values
‚îî‚îÄ‚îÄ terraform.tfvars    # Your configuration (not committed)
```

### Configuration

1. Copy the example file:
```bash
cd terraform
cp terraform.tfvars.example terraform.tfvars
```

2. Edit `terraform.tfvars` with your GCP project:
```hcl
project_id  = "your-gcp-project-id"
region      = "us-central1"
environment = "dev"
```

### Deploy

```bash
# Authenticate with GCP
gcloud auth application-default login

# Enable required APIs
gcloud services enable storage.googleapis.com iam.googleapis.com

# Initialize Terraform
terraform init

# Preview changes
terraform plan

# Apply (creates resources)
terraform apply

# Destroy when done (optional)
terraform destroy
```

### Resources Created

| Resource | Description |
|----------|-------------|
| `google_storage_bucket.bronze` | GCS bucket for raw/bronze data |
| `google_storage_bucket.gold` | GCS bucket for curated/gold data |
| `google_service_account.pipeline` | Service account for pipeline |
| `google_storage_bucket_iam_member` | IAM bindings for bucket access |

## üß™ Testing

```bash
# Run all tests
make test

# Run unit tests only
pytest tests/unit/ -v

# Run integration tests (requires PySpark)
pytest tests/integration/ -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

### Test Structure

```
tests/
‚îú‚îÄ‚îÄ unit/                    # Fast, isolated tests
‚îÇ   ‚îú‚îÄ‚îÄ test_schemas.py     # Schema validation
‚îÇ   ‚îî‚îÄ‚îÄ test_config.py      # Configuration tests
‚îú‚îÄ‚îÄ integration/            # End-to-end tests
‚îÇ   ‚îî‚îÄ‚îÄ test_pipeline.py    # Full pipeline tests with Spark
‚îî‚îÄ‚îÄ conftest.py             # Shared fixtures
```

## üìä Monitoring & Alerting

The pipeline includes production-grade monitoring capabilities:

### Quick Commands

```bash
# Full health report (requires Docker running)
make monitor

# Demo the PipelineMonitor (no DB required)
make monitor-demo

# Local monitoring (requires POSTGRES_HOST=localhost)
make monitor-local
```

### Structured Logging

```python
from src.monitoring import get_logger

logger = get_logger(__name__)
logger.info("Processing started", extra={"extra_fields": {"records": 1000}})
```

Outputs JSON logs compatible with CloudWatch, Stackdriver, ELK:
```json
{"timestamp": "2023-01-15T10:30:00Z", "level": "INFO", "message": "Processing started", "records": 1000}
```

### Pipeline Metrics

```python
from src.monitoring import PipelineMonitor

monitor = PipelineMonitor("bronze_ingestion")

with monitor.track_stage("download") as stage:
    data = download_data()
    stage.records_out = len(data)

with monitor.track_stage("transform") as stage:
    stage.records_in = len(data)
    result = transform(data)
    stage.records_out = len(result)
    stage.records_rejected = len(data) - len(result)

monitor.report()  # Prints execution summary
```

### Automatic Alerting

Alerts are triggered automatically when:
- **WARNING**: Rejection rate > 5%
- **CRITICAL**: Rejection rate > 20%
- **CRITICAL**: Stage failure/exception

### Data Quality Checks

```python
from src.monitoring import DataQualityMonitor

dq = DataQualityMonitor(df, "trips_data")
dq.check_nulls(["fare_amount", "trip_distance"])
dq.check_range("fare_amount", min_val=0, max_val=500)
dq.check_uniqueness("trip_id")

report = dq.get_report()
```

## üîÑ CI/CD

GitHub Actions workflow runs on every push:

1. **Lint**: flake8, black, isort
2. **Unit Tests**: pytest for schemas, config
3. **Integration Tests**: Full Spark pipeline tests
4. **Terraform**: fmt + validate
5. **Docker**: Build image
6. **Security**: Dependency vulnerability scan

### Pipeline Status

All jobs run automatically on push to `main` or `develop` branches.

## üìÖ Historical Data Strategy

### Processing Multiple Months

```bash
make run-full MONTHS="2023-01 2023-02 2023-03"
```

### Backfill Specific Month

```bash
make run-backfill MONTHS=2023-02
```

### Idempotency

- Uses partition overwrite mode
- Re-running for a month replaces only that partition
- No duplicates, no data corruption

## ‚ö†Ô∏è Considerations

### Current Limitations

- Single-node Spark (Docker) for development
- No real-time streaming (batch only)
- Alerting is log-based (extensible to Slack/PagerDuty)

### Implemented ‚úÖ

- [x] Structured JSON logging for log aggregators
- [x] Pipeline monitoring with metrics
- [x] Automatic alerting on data quality issues
- [x] Unit and integration tests
- [x] CI/CD with GitHub Actions
- [x] Infrastructure as Code (Terraform)


